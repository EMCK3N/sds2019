{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 7: Data structuring  3\n",
    "\n",
    "*Morning, August 15, 2019*\n",
    "\n",
    "In this Exercise Set we finalize our work with the weather data we started working on in Exercise Set 4. We will also study a dataset of traffic data from Copenhagen to iterate through the pandas workflow once more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "# Increases the plot size a little\n",
    "mpl.rcParams['figure.figsize'] = 11, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 7.1: Weather data, part 3\n",
    "We continue with the final part of three exercises on structuring weather data. In this exercise you must use the function for fetching and structuring weather data which you made in Exercise 6.1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.1:** Plot the monthly max,min, mean, first and third quartiles for maximum temperature for our station with the ID _'ITE00100550'_ in 1864. \n",
    "\n",
    "> *Hint*: the method `describe` computes all these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station identifier</th>\n",
       "      <th>date</th>\n",
       "      <th>observation type</th>\n",
       "      <th>observation value</th>\n",
       "      <th>TMAX_F</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-02</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>8</td>\n",
       "      <td>46.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-03</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-28</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-04</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-05</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-19</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-06</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-13</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-07</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-4</td>\n",
       "      <td>24.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-08</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>13</td>\n",
       "      <td>55.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-09</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>13</td>\n",
       "      <td>55.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-10</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>6</td>\n",
       "      <td>42.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>ITE00100550</td>\n",
       "      <td>1864-01-11</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    station identifier       date observation type  observation value  TMAX_F  \\\n",
       "74         ITE00100550 1864-01-02             TMAX                  8    46.4   \n",
       "151        ITE00100550 1864-01-03             TMAX                -28   -18.4   \n",
       "226        ITE00100550 1864-01-04             TMAX                  0    32.0   \n",
       "304        ITE00100550 1864-01-05             TMAX                -19    -2.2   \n",
       "382        ITE00100550 1864-01-06             TMAX                -13     8.6   \n",
       "459        ITE00100550 1864-01-07             TMAX                 -4    24.8   \n",
       "537        ITE00100550 1864-01-08             TMAX                 13    55.4   \n",
       "617        ITE00100550 1864-01-09             TMAX                 13    55.4   \n",
       "694        ITE00100550 1864-01-10             TMAX                  6    42.8   \n",
       "769        ITE00100550 1864-01-11             TMAX                -15     5.0   \n",
       "\n",
       "     month  \n",
       "74       1  \n",
       "151      1  \n",
       "226      1  \n",
       "304      1  \n",
       "382      1  \n",
       "459      1  \n",
       "537      1  \n",
       "617      1  \n",
       "694      1  \n",
       "769      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to Ex. 7.1.1]\n",
    "url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/1864.csv.gz'\n",
    "path = '/Users/michaelemcken/Google Drive/Python/sds2019/material/session_4/my_weather.csv'\n",
    "df = pd.read_csv(path, sep=',')\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "\n",
    "\n",
    "df_ITE = df.loc[(df['station identifier'] == 'ITE00100550') & (df['observation type'] == 'TMAX')]\n",
    "\n",
    "df_ITE.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      observation value                                 \n",
      "                    max  min        mean      q1      q2\n",
      "month                                                   \n",
      "1                    53  -63   -1.133333  -23.50   22.00\n",
      "2                    84  -18   41.344828   15.00   68.00\n",
      "3                   180   79  122.000000  100.50  141.50\n",
      "4                   251   63  162.433333  144.25  184.00\n",
      "5                   270  140  214.709677  195.00  238.00\n",
      "6                   313  204  266.700000  251.00  285.00\n",
      "7                   325  254  293.870968  284.50  309.00\n",
      "8                   348  210  289.225806  264.50  316.00\n",
      "9                   299  180  241.033333  215.00  268.00\n",
      "10                  205  119  165.193548  150.00  183.50\n",
      "11                  146   50   97.033333   76.75  113.75\n",
      "12                   93   15   53.161290   36.00   70.00\n"
     ]
    }
   ],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "def q2(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "split_vars = ['month'] \n",
    "apply_vars = ['observation value']\n",
    "apply_fcts = ['max', 'min', 'mean', q1, q2]\n",
    "\n",
    "stats = df_ITE\\\n",
    "    .groupby(split_vars)\\\n",
    "    [apply_vars]\\\n",
    "    .agg(apply_fcts)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = sns.boxplot(x = stats.index, y = df['observation value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.2:** Get the processed data from years 1864-1867 as a list of DataFrames. Convert the list into a single DataFrame by concatenating vertically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/1864.csv.gz\n",
      "   ITE00100550  18640101  TMAX   10 Unnamed: 4 Unnamed: 5  E  Unnamed: 7\n",
      "0  ITE00100550  18640101  TMIN  -23        NaN        NaN  E         NaN\n",
      "1  ITE00100550  18640101  PRCP   25        NaN        NaN  E         NaN\n",
      "2  ASN00079028  18640101  PRCP    0        NaN        NaN  a         NaN\n",
      "3  USC00064757  18640101  PRCP  119        NaN        NaN  F         NaN\n",
      "4  SF000208660  18640101  PRCP    0        NaN        NaN  I         NaN\n",
      "5  ASN00089000  18640101  PRCP    0        NaN        NaN  a         NaN\n",
      "6  SWE00100003  18640101  PRCP    0        NaN        NaN  E         NaN\n",
      "7  ASN00086071  18640101  TMAX  214        NaN        NaN  a         NaN\n",
      "8  ASN00086071  18640101  TMIN  101        NaN        NaN  a         NaN\n",
      "9  ASN00086071  18640101  PRCP    0        NaN        NaN  a         NaN\n",
      "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/1865.csv.gz\n",
      "   ITE00100550  18650101  TMAX   20 Unnamed: 4 Unnamed: 5  E  Unnamed: 7\n",
      "0  ITE00100550  18650101  TMIN  -30        NaN        NaN  E         NaN\n",
      "1  ITE00100550  18650101  PRCP    0        NaN        NaN  E         NaN\n",
      "2  USC00064757  18650101  PRCP    0        NaN        NaN  F         NaN\n",
      "3  SF000208660  18650101  PRCP    0        NaN        NaN  I         NaN\n",
      "4  ASN00089000  18650101  PRCP    0        NaN        NaN  a         NaN\n",
      "5  SWE00100003  18650101  PRCP    0        NaN        NaN  E         NaN\n",
      "6  ASN00086071  18650101  TMAX  239        NaN        NaN  a         NaN\n",
      "7  ASN00086071  18650101  TMIN  151        NaN        NaN  a         NaN\n",
      "8  ASN00086071  18650101  PRCP    0        NaN        NaN  a         NaN\n",
      "9  USP00CA0003  18650101  PRCP  132        NaN        NaN  F         NaN\n",
      "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/1866.csv.gz\n",
      "   ITE00100550  18660101  TMAX    5 Unnamed: 4 Unnamed: 5  E  Unnamed: 7\n",
      "0  ITE00100550  18660101  TMIN  -43        NaN        NaN  E         NaN\n",
      "1  ITE00100550  18660101  PRCP    0        NaN        NaN  E         NaN\n",
      "2  USC00064757  18660101  PRCP    0        NaN        NaN  F         NaN\n",
      "3  SF000208660  18660101  PRCP    0        NaN        NaN  I         NaN\n",
      "4  ASN00089000  18660101  PRCP    0        NaN        NaN  a         NaN\n",
      "5  CA006150689  18660101  TMAX   28        NaN        NaN  C         NaN\n",
      "6  CA006150689  18660101  TMIN -117        NaN        NaN  C         NaN\n",
      "7  CA006150689  18660101  PRCP    0        NaN        NaN  C         NaN\n",
      "8  CA006150689  18660101  SNOW    0        NaN        NaN  C         NaN\n",
      "9  SWE00100003  18660101  PRCP    0        NaN        NaN  E         NaN\n",
      "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/1867.csv.gz\n",
      "   ITE00100550  18670101  TMAX   60 Unnamed: 4 Unnamed: 5  E  Unnamed: 7\n",
      "0  ITE00100550  18670101  TMIN   -7        NaN        NaN  E         NaN\n",
      "1  ITE00100550  18670101  PRCP    0        NaN        NaN  E         NaN\n",
      "2  USC00064757  18670101  PRCP    0        NaN        NaN  F         NaN\n",
      "3  USC00064757  18670101  SNOW  127        NaN        NaN  F         NaN\n",
      "4  ASN00017003  18670101  PRCP    0        NaN        NaN  a         NaN\n",
      "5  USC00264939  18670101  PRCP    0        NaN        NaN  F         NaN\n",
      "6  SF000208660  18670101  PRCP    0        NaN        NaN  I         NaN\n",
      "7  ASN00089000  18670101  PRCP    0        NaN        NaN  a         NaN\n",
      "8  CA006150689  18670101  TMAX  -17        NaN        NaN  C         NaN\n",
      "9  CA006150689  18670101  TMIN  -78        NaN        NaN  C         NaN\n"
     ]
    }
   ],
   "source": [
    "# [Answer to Ex. 7.1.2]\n",
    "init_url = 'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/'\n",
    "file_type = '.csv.gz'\n",
    "init_name = 'my_weather_'\n",
    "save_file_type = '.csv'\n",
    "i = 1864 \n",
    "\n",
    "while i <= 1867:\n",
    "    # Creating URL\n",
    "    url = init_url + str(i) + file_type\n",
    "    print(url)\n",
    "    # Reading CSV file, to check if the load was succesful\n",
    "    df_i = pd.read_csv(url, sep=',')\n",
    "    print(df_i.head(10))\n",
    "    # Saving the CSV file to local computer\n",
    "    file_name = init_name + str(i) + save_file_type\n",
    "    df_i.to_csv(file_name, index=False)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1864 = '/Users/michaelemcken/Google Drive/Python/sds2019/material/session_7/my_weather_1864.csv'\n",
    "path_1865 = '/Users/michaelemcken/Google Drive/Python/sds2019/material/session_7/my_weather_1865.csv'\n",
    "path_1866 = '/Users/michaelemcken/Google Drive/Python/sds2019/material/session_7/my_weather_1866.csv'\n",
    "path_1867 = '/Users/michaelemcken/Google Drive/Python/sds2019/material/session_7/my_weather_1867.csv'\n",
    "df_1864 = pd.read_csv(path_1864, sep=',')\n",
    "df_1865 = pd.read_csv(path_1865, sep=',')\n",
    "df_1866 = pd.read_csv(path_1866, sep=',')\n",
    "df_1867 = pd.read_csv(path_1867, sep=',')\n",
    "\n",
    "df_columns = ['station identifier', 'date', 'observation type', 'observation value', 'NaN', 'NaN', 'NaN', 'NaN']\n",
    "df_1864.columns = df_columns\n",
    "df_1865.columns = df_columns\n",
    "df_1866.columns = df_columns\n",
    "df_1867.columns = df_columns\n",
    "\n",
    "#df_1864 = df_1864.dropna\n",
    "#df_1864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      station identifier      date observation type  observation value  NaN  \\\n",
      "27343        UK000056225  18641231             PRCP                  3  NaN   \n",
      "27344        ASN00026026  18641231             PRCP                  0  NaN   \n",
      "27345        ASN00089049  18641231             PRCP                  0  NaN   \n",
      "27346        SZ000006717  18641231             TMAX                -62  NaN   \n",
      "27347        SZ000006717  18641231             TMIN               -105  NaN   \n",
      "\n",
      "       NaN  NaN  NaN  \n",
      "27343  NaN    E  NaN  \n",
      "27344  NaN    a  NaN  \n",
      "27345  NaN    a  NaN  \n",
      "27346  NaN    G  NaN  \n",
      "27347  NaN    G  NaN  \n",
      "      station identifier      date observation type  observation value  NaN  \\\n",
      "28673        UK000056225  18651231             PRCP                 79  NaN   \n",
      "28674        ASN00026026  18651231             PRCP                  0  NaN   \n",
      "28675        ASN00089049  18651231             PRCP                  0  NaN   \n",
      "28676        SZ000006717  18651231             TMAX                -66  NaN   \n",
      "28677        SZ000006717  18651231             TMIN                -84  NaN   \n",
      "\n",
      "       NaN  NaN  NaN  \n",
      "28673  NaN    E  NaN  \n",
      "28674  NaN    a  NaN  \n",
      "28675  NaN    a  NaN  \n",
      "28676  NaN    G  NaN  \n",
      "28677  NaN    G  NaN  \n",
      "      station identifier      date observation type  observation value  NaN  \\\n",
      "37841        ASN00089049  18661231             PRCP                  0  NaN   \n",
      "37842        SZ000006717  18661231             TMAX                -49  NaN   \n",
      "37843        SZ000006717  18661231             TMIN                -92  NaN   \n",
      "37844        ASN00023078  18661231             PRCP                  0  NaN   \n",
      "37845        USC00053010  18661231             PRCP                  8  NaN   \n",
      "\n",
      "       NaN  NaN  NaN  \n",
      "37841  NaN    a  NaN  \n",
      "37842  NaN    G  NaN  \n",
      "37843  NaN    G  NaN  \n",
      "37844  NaN    a  NaN  \n",
      "37845  NaN    F  NaN  \n",
      "      station identifier      date observation type  observation value  NaN  \\\n",
      "47843        USC00117391  18671231             PRCP                  0  NaN   \n",
      "47844        ASN00089049  18671231             PRCP                 20  NaN   \n",
      "47845        SZ000006717  18671231             TMAX               -158  NaN   \n",
      "47846        SZ000006717  18671231             TMIN               -199  NaN   \n",
      "47847        ASN00071010  18671231             PRCP                 43  NaN   \n",
      "\n",
      "       NaN  NaN  NaN  \n",
      "47843  NaN    F  NaN  \n",
      "47844  NaN    a  NaN  \n",
      "47845  NaN    G  NaN  \n",
      "47846  NaN    G  NaN  \n",
      "47847  NaN    a  NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df_1864.tail())\n",
    "print(df_1865.tail())\n",
    "print(df_1866.tail())\n",
    "print(df_1867.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       station identifier      date observation type  observation value  NaN  \\\n",
      "0             ITE00100550  18640101             TMIN                -23  NaN   \n",
      "1             ITE00100550  18640101             PRCP                 25  NaN   \n",
      "2             ASN00079028  18640101             PRCP                  0  NaN   \n",
      "3             USC00064757  18640101             PRCP                119  NaN   \n",
      "4             SF000208660  18640101             PRCP                  0  NaN   \n",
      "5             ASN00089000  18640101             PRCP                  0  NaN   \n",
      "6             SWE00100003  18640101             PRCP                  0  NaN   \n",
      "7             ASN00086071  18640101             TMAX                214  NaN   \n",
      "8             ASN00086071  18640101             TMIN                101  NaN   \n",
      "9             ASN00086071  18640101             PRCP                  0  NaN   \n",
      "10            USP00CA0003  18640101             PRCP                  0  NaN   \n",
      "11            USC00189674  18640101             PRCP                  0  NaN   \n",
      "12            USC00144559  18640101             PRCP                  0  NaN   \n",
      "13            USC00144559  18640101             SNOW                  0  NaN   \n",
      "14            CA006158350  18640101             TMAX                 11  NaN   \n",
      "15            CA006158350  18640101             TMIN               -133  NaN   \n",
      "16            CA006158350  18640101             PRCP                  5  NaN   \n",
      "17            CA006158350  18640101             SNOW                  5  NaN   \n",
      "18            HRE00105189  18640101             PRCP                189  NaN   \n",
      "19            ASN00067054  18640101             PRCP                 61  NaN   \n",
      "20            ASN00081003  18640101             PRCP                  0  NaN   \n",
      "21            ASN00078037  18640101             PRCP                  0  NaN   \n",
      "22            ASN00070037  18640101             PRCP                  0  NaN   \n",
      "23            EIE00101859  18640101             TMAX                 94  NaN   \n",
      "24            EIE00101859  18640101             TMIN                 11  NaN   \n",
      "25            EIE00101859  18640101             PRCP                 82  NaN   \n",
      "26            ASN00040214  18640101             PRCP                  3  NaN   \n",
      "27            BE000006447  18640101             TMAX                -23  NaN   \n",
      "28            BE000006447  18640101             TMIN                -36  NaN   \n",
      "29            AGE00135039  18640101             PRCP                  0  NaN   \n",
      "...                   ...       ...              ...                ...  ...   \n",
      "141690        CA006122845  18671231             TMIN               -128  NaN   \n",
      "141691        CA006122845  18671231             PRCP                 25  NaN   \n",
      "141692        CA006122845  18671231             SNOW                 25  NaN   \n",
      "141693        ITE00105250  18671231             PRCP                 16  NaN   \n",
      "141694        ASN00067015  18671231             PRCP                  0  NaN   \n",
      "141695        CA006148100  18671231             TMAX                -33  NaN   \n",
      "141696        CA006148100  18671231             TMIN               -144  NaN   \n",
      "141697        CA006148100  18671231             PRCP                  0  NaN   \n",
      "141698        CA006148100  18671231             SNOW                  0  NaN   \n",
      "141699        ASN00074128  18671231             TMAX                222  NaN   \n",
      "141700        ASN00074128  18671231             TMIN                100  NaN   \n",
      "141701        ASN00074128  18671231             PRCP                  0  NaN   \n",
      "141702        CA006101872  18671231             TMAX                -39  NaN   \n",
      "141703        CA006101872  18671231             TMIN               -200  NaN   \n",
      "141704        CA006101872  18671231             PRCP                 66  NaN   \n",
      "141705        CA006101872  18671231             SNOW                 66  NaN   \n",
      "141706        CA006137735  18671231             TMAX                -11  NaN   \n",
      "141707        CA006137735  18671231             TMIN               -139  NaN   \n",
      "141708        CA006137735  18671231             PRCP                  0  NaN   \n",
      "141709        CA006137735  18671231             SNOW                  0  NaN   \n",
      "141710        ASN00090015  18671231             TMAX                167  NaN   \n",
      "141711        UK000056225  18671231             TMAX                 -5  NaN   \n",
      "141712        UK000056225  18671231             TMIN                -23  NaN   \n",
      "141713        UK000056225  18671231             PRCP                  5  NaN   \n",
      "141714        ASN00026026  18671231             PRCP                  0  NaN   \n",
      "141715        USC00117391  18671231             PRCP                  0  NaN   \n",
      "141716        ASN00089049  18671231             PRCP                 20  NaN   \n",
      "141717        SZ000006717  18671231             TMAX               -158  NaN   \n",
      "141718        SZ000006717  18671231             TMIN               -199  NaN   \n",
      "141719        ASN00071010  18671231             PRCP                 43  NaN   \n",
      "\n",
      "        NaN  NaN  NaN  \n",
      "0       NaN    E  NaN  \n",
      "1       NaN    E  NaN  \n",
      "2       NaN    a  NaN  \n",
      "3       NaN    F  NaN  \n",
      "4       NaN    I  NaN  \n",
      "5       NaN    a  NaN  \n",
      "6       NaN    E  NaN  \n",
      "7       NaN    a  NaN  \n",
      "8       NaN    a  NaN  \n",
      "9       NaN    a  NaN  \n",
      "10      NaN    F  NaN  \n",
      "11      NaN    F  NaN  \n",
      "12      NaN    F  NaN  \n",
      "13      NaN    F  NaN  \n",
      "14      NaN    C  NaN  \n",
      "15      NaN    C  NaN  \n",
      "16      NaN    C  NaN  \n",
      "17      NaN    C  NaN  \n",
      "18      NaN    E  NaN  \n",
      "19      NaN    a  NaN  \n",
      "20      NaN    a  NaN  \n",
      "21      NaN    a  NaN  \n",
      "22      NaN    a  NaN  \n",
      "23      NaN    E  NaN  \n",
      "24      NaN    E  NaN  \n",
      "25      NaN    E  NaN  \n",
      "26      NaN    a  NaN  \n",
      "27      NaN    I  NaN  \n",
      "28      NaN    I  NaN  \n",
      "29      NaN    E  NaN  \n",
      "...     ...  ...  ...  \n",
      "141690  NaN    C  NaN  \n",
      "141691  NaN    C  NaN  \n",
      "141692  NaN    C  NaN  \n",
      "141693  NaN    E  NaN  \n",
      "141694  NaN    a  NaN  \n",
      "141695  NaN    C  NaN  \n",
      "141696  NaN    C  NaN  \n",
      "141697  NaN    C  NaN  \n",
      "141698  NaN    C  NaN  \n",
      "141699  NaN    a  NaN  \n",
      "141700  NaN    a  NaN  \n",
      "141701  NaN    a  NaN  \n",
      "141702  NaN    C  NaN  \n",
      "141703  NaN    C  NaN  \n",
      "141704  NaN    C  NaN  \n",
      "141705  NaN    C  NaN  \n",
      "141706  NaN    C  NaN  \n",
      "141707  NaN    C  NaN  \n",
      "141708  NaN    C  NaN  \n",
      "141709  NaN    C  NaN  \n",
      "141710  NaN    a  NaN  \n",
      "141711  NaN    E  NaN  \n",
      "141712  NaN    E  NaN  \n",
      "141713  NaN    E  NaN  \n",
      "141714  NaN    a  NaN  \n",
      "141715  NaN    F  NaN  \n",
      "141716  NaN    a  NaN  \n",
      "141717  NaN    G  NaN  \n",
      "141718  NaN    G  NaN  \n",
      "141719  NaN    a  NaN  \n",
      "\n",
      "[141720 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "weather_total = pd.concat([df_1864, df_1865, df_1866, df_1867], join='inner', axis=0, sort=False)\n",
    "weather_total.index = pd.RangeIndex(len(weather_total.index))\n",
    "print(weather_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.1.3:** Parse the station location data which you can find at https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt. Merge station locations onto the weather data spanning 1864-1867.  \n",
    "\n",
    "> _Hint:_ The location data have the folllowing format, \n",
    "\n",
    "```\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSN FLAG     73-75   Character\n",
    "HCN/CRN FLAG 77-79   Character\n",
    "WMO ID       81-85   Character\n",
    "------------------------------\n",
    "```\n",
    "\n",
    "> *Hint*: The station information has fixed width format - does there exist a pandas reader for that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.1.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 7.2: Traffic data in Copenhagen\n",
    "\n",
    "In this second part of exercise set 7 you will be working with traffic data from Copenhagen Municipality.\n",
    "\n",
    "The municipality have made the data openly available through the [opendata.dk](http://www.opendata.dk/) platform. We will use the data from traffic counters to construct a dataset of hourly traffic. We will use this data to get basic insights on the development in traffic over time and relate it to weather. The gist here is to practice a very important skill in Data Science: being able to quickly fetch data from the web and structure it so that you can work with it. Scraping usually gets a bit more advanced than what we will do today, but the following exercises should give you a taste for how it works. The bulk of these exercise, however, revolve around using the Pandas library to structure and analyze data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7.2.a: getting some data to work with\n",
    "\n",
    "Hence follows a simple scraping exercise where you (1) collect urls for datasets in the webpage listing data on traffic counters and (2) use these urls to load the data into one dataframe.\n",
    "\n",
    "> **Ex. 7.2.1:** Using the requests module, extract the html markup of the webpage data.kk.dk/dataset/faste-trafiktaellinger and store it as a string in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.2:** Using the re module, extract a list of all the urls in the html string and store them in a new variable.\n",
    "\n",
    "> _Hint:_ Try using the re.findall method. You may want to Google around to figure out how to do this. Protip: searching for something along the lines of \"extract all links in html regex python\" and hitting the first StackOverflow link will probably get you farther than reading elaborate documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.3:** Create a new variable that only contains the links that point to downloadable traffic data sheets. \n",
    "\n",
    "> _Hint:_ You want to filter the results from above. For example to only include urls with the term 'download' in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.4:** Using pd.read_excel method, load the datasets into a list. Your resulting variable should hold a list of Pandas dataframes.\n",
    "\n",
    "> _Hint:_ you may want to set the skiprows= keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.5:** Merge the list of dataframes into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7.2.b Structuring your data\n",
    "\n",
    "If you successfully completed the previous part, you should now have a dataframe with about 183.397 rows (if your number of rows is close but not the same, worry not—it matters little in the following). Well done! But the data is still in no shape for analysis, so we must clean it up a little.\n",
    "\n",
    "161.236 rows (and 30 columns) is a lot of data. ~3.3 MB by my back-of-the-envelope calculations, so not \"Big Data\", but still enough to make your CPU heat up if you don't use it carefully. Pandas is built to handle fairly large dataframes and has advanced functionality to perform very fast operations even when the size of your data grows huge. So instead of working with basic Python we recommend working pandas built-in procedures as they are constructed to be fast on dataframes.\n",
    "\n",
    "Nerd fact: the reason pandas is much faster than pure Python is that dataframes access a lower level programming languages (namely C, C++) which are multiple times faster than Python. The reason it is faster is that it has a higher level of explicitness and thus is more difficult to learn and navigate.\n",
    "\n",
    "> **Ex. 7.2.6:** Reset the row indices of your dataframe so the first index is 0 and the last is whatever the number of rows your dataframe has. Also drop the column named 'index' and the one named `Spor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.7:** Rename variables from Danish to English using the dictionary below.\n",
    "\n",
    "```python \n",
    "dk_to_uk = {\n",
    "    'Vejnavn':'road_name',\n",
    "    '(UTM32)':'UTM32_north',\n",
    "    '(UTM32).1':'UTM32_east',\n",
    "    'Dato':'date',\n",
    "    'Vej-Id':'road_id'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is quite efficient. For example, when you create a new dataframe by manipulating an old one, Python notices that—apart from some minor changes—these two objects are almost the same. Since memory is a precious resource, Python will represent the values in the new dataframe as references to the variables in the old dataset. This is great for performance, but if you for whatever reason change some of the values in your old dataframe, values in the new one will also change—and we don't want that! Luckily, we can break this dependency.\n",
    "\n",
    "> **Ex. 7.2.8:** Break the dependencies of the dataframe that resulted from Ex. 7.2.7 using the `.copy` method. Delete all other dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have structured appropriately, something that you will want to do again and again is selecting subsets of the data. Specifically, it means that you select specific rows in the dataset based on some column values.\n",
    "\n",
    ">**Ex. 7.2.9:** Create a new column in the dataframe called total that is True when the last letter of road_id is T and otherwise False.\n",
    "\n",
    "> _Hint:_ you will need the `pd.Series.str` attribute for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.10:** Select rows where total is True. Delete all the remaining observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.11:** Make two datasets based on the lists of columns below. Call the dataset with spatial columns data_geo and the other data.\n",
    "\n",
    "```python\n",
    "# Columns for `geo_data`, stored in `geo_columns`\n",
    "spatial_columns = ['road_name', 'UTM32_north', 'UTM32_east']\n",
    "\n",
    "# Columns for `data`, stored in `select_columns`\n",
    "hours = ['kl.{}-{}'.format(str(h).zfill(2), str(h+1).zfill(2)) for h in range(24)]\n",
    "select_columns = ['road_name', 'date'] + hours\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.12:** Drop the duplicate rows in data_geo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formatting: wide and narrow format**\n",
    "\n",
    "When talking about two-dimensional data (matrices, tables or dataframes, we can call it many things), we can either say that it is in wide or long format (see explanation here, \"wide\" and \"long\" are used interchangably). In Pandas we can use the commands stack and unstack to move between these formats.\n",
    "\n",
    "The wide format has the advantage that it often requires less storage and is easier to read when printed. On the other hand the long format can be easier for modelling, because each observation has its own row. Turns out that the latter is what we most often need.\n",
    "\n",
    "> **Ex. 7.2.13:** Turn the dataset from wide to long so hourly data is now vertically stacked. Store this dataset in a dataframe called data. Name the column with hourly information hour_period. \n",
    "\n",
    "> _Hint:_ pandas' melt function may be of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical data**\n",
    "\n",
    "Categorical data can contain Python objects, usually strings. These are smart if you have variables with string observations that are long and often repeated, e.g. with road names.\n",
    "\n",
    "> **Ex. 7.2.14:** Use the `.astype` method to convert the type of the road_name column to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure temporal data\n",
    "\n",
    "Pandas has native support for working with temporal data. This is handy as much 'big data' often has time stamps which we can make Pandas aware of. Once we have encoded temporal data it can be used to extract information such as the hour, second etc.\n",
    "\n",
    "> **Ex. 7.2.15:** Create a new column called hour which contains the hour-of-day for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 7.2.16:** Create a new column called time, that contains the time of the row in datetime format. Delete the old temporal columns (hour, hour_period, date) to save memory.\n",
    "\n",
    "> _Hint:_ try making an intermediary series of strings that has all temporal information for the row; then use pandas to_datetime function where you can specify the format of the date string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.17:** Using your time column make a new column called weekday which stores the weekday (in values between 0 and 6) of the corresponding datetime.\n",
    "\n",
    "> _Hint:_ try using the dt method for the series called time; dt has some relevant methods itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical descriptions of traffic data\n",
    "\n",
    "> **Ex. 7.2.18:** Print the \"descriptive statistics\" of the traffic column. Also show a kernel density estimate of the values.\n",
    "\n",
    "> _Hint:_ Use the describe method of pandas dataframes for the first task. Use seaborn for the second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.19:** Which road has the most average traffic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 7.2.20:** Compute annual, average road traffic during day hours (9-17). Which station had the least traffic in 2013? Which station has seen highest growth in traffic from 2013 to 2014?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 7.2.19]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
