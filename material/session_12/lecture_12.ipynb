{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Notebook contents: \n",
    "\n",
    "This notebook contains a lecture. The code for generating plots are found at the of the notebook. Links below.\n",
    "\n",
    "- [presentation](#Session-1b:)\n",
    "- [code for plots](#Code-for-plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 12:\n",
    "## Supervised learning, part 1\n",
    "\n",
    "*Andreas Bjerre-Nielsen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "1. [Modelling data](#Modelling-data)\n",
    "1. [A familiar regression model](#A-familiar-regression-model)\n",
    "1. [The curse of overfitting](#The-curse-of-overfitting)\n",
    "1. [Important details](#Implementation-details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## It sucks not being able to complete all the exercises..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We know, we feel sorry, we have been there. The exercises help you grow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> #### Hadley Wickham\n",
    "\n",
    "> The bad news is that when ever you learn a new skill you’re going to suck. It’s going to be frustrating. The good news is that is typical and happens to everyone and it is only temporary. You can’t go from knowing nothing to becoming an expert without going through a period of great frustration and great suckiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> #### Kosuke Imai\n",
    "\n",
    "> One can learn data analysis only by doing, not by reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vaaaamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised problems (1)\n",
    "*How do we distinguish between problems?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_identify_question' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9693da4ebe88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf_identify_question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'f_identify_question' is not defined"
     ]
    }
   ],
   "source": [
    "f_identify_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised problems (2)\n",
    "*The two canonical problems*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_identify_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised problems (3)\n",
    "*Which models have we seen for classification?*\n",
    "\n",
    "- .\n",
    "\n",
    "- .\n",
    "\n",
    "- ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model complexity (1)\n",
    "*What does a model of low complexity look like?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f_complexity[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model complexity (2)\n",
    "*What does medium model complexity look like?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f_complexity[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model complexity (3)\n",
    "*What does high model complexity look like?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f_complexity[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model fitting (1)\n",
    "*Quiz (1 min.): Which model fitted the data best?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f_bias_var['regression'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model fitting (2)\n",
    "*What does underfitting and overfitting look like for classification?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f_bias_var['classification'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two agendas (1)\n",
    "\n",
    "What are the objectives of empirical research? \n",
    "\n",
    "1. *causation*: what is the effect of a particular variable on an outcome? \n",
    "2. *prediction*: find some function that provides a good prediction of $y$ as a function of $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two agendas (2)\n",
    "\n",
    "How might we express the agendas in a model?\n",
    "\n",
    "$$ y = \\alpha + \\beta x + \\varepsilon $$\n",
    "\n",
    "- *causation*: interested in $\\hat{\\beta}$ \n",
    "\n",
    "- *prediction*: interested in $\\hat{y}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two agendas (3)\n",
    "\n",
    "Might these two agendas be related at a deeper level? \n",
    "\n",
    "Can prediction quality inform us about how to make causal models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A familiar regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation (1)\n",
    "*Do we know already some ways to estimate regression models?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Social scientists know all about the Ordinary Least Squares (OLS).\n",
    "- Some properties of OLS\n",
    "    - Is applied to solve linear models.\n",
    "    - Estimates both parameters and their standard deviation.\n",
    "    - Is the best linear unbiased estimator under regularity conditions.     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*How is OLS estimated?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\beta=(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}$\n",
    "- computation requires non perfect multicollinarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation (2)\n",
    "*How might we estimate a linear regression model?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- first order method (e.g. gradient descent)\n",
    "- second order method (e.g. Newton-Raphson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*So what the hell was gradient descent?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- repeat the following: compute errors, multiply with features, and update coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation (3)\n",
    "*Can you explain that in details?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yes, like with Adaline, we minimize the sum of squared errors (SSE): \n",
    "\\begin{align}SSE&=\\boldsymbol{e}^{T}\\boldsymbol{e}\\\\\\boldsymbol{e}&=\\textbf{y}-\\textbf{X}\\textbf{w}\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X = np.random.normal(size=(3,2))\n",
    "y = np.random.normal(size=(3))\n",
    "w = np.random.normal(size=(3))\n",
    "\n",
    "e = y-(w[0]+X.dot(w[1:]))\n",
    "SSE = e.T.dot(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation (4)\n",
    "*And what about the updating..? What is it something about the first order deritative?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial SSE}{\\partial\\hat{w}}=&\\textbf{X}^T\\textbf{e},\\\\\n",
    " \\Delta\\hat{w}=&\\eta\\cdot\\textbf{X}^T\\textbf{e}=\\eta\\cdot\\textbf{X}^T(\\textbf{y}-\\hat{\\textbf{y}})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "eta = 0.001 # learning rate\n",
    "fod = X.T.dot(e)\n",
    "update_vars = eta*fod\n",
    "update_bias = eta*e.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation (5)\n",
    "*What might some advantages be relative to OLS?*\n",
    "\n",
    "- OLS \n",
    "    - Computation complexity $\\mathcal{O}(K^2N)$  ([read more](https://math.stackexchange.com/questions/84495/computational-complexity-of-least-square-regression-operation))\n",
    "        - Quadratic scaling in number of variables ($K$) is slow!\n",
    "- Gradient descent (GD)\n",
    "    - Works despite high multicollinarity\n",
    "    - Scales well: can be applied in subsets to very large datasets \n",
    "        - We only need subset in memory        \n",
    "    - Note: not guaranteed convergence time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a polynomial (1)\n",
    "Polyonomial: $f(x) = 2+8*x^4$\n",
    "\n",
    "Try models of increasing order polynomials. \n",
    "\n",
    "- Split data into train and test (50/50)\n",
    "\n",
    "\n",
    "- For polynomial order 0 to 9:\n",
    "    - Iteration n: $y = \\sum_{k=0}^{n}(\\beta_k\\cdot x^k)+\\varepsilon$.\n",
    "    - Estimate order n model on training data\n",
    "    - Evaluate with on test data with RMSE: \n",
    "        - $log RMSE = \\log (\\sqrt{MSE})$     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a polynomial (2)\n",
    "We generate samples of data from true model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def true_fct(X):\n",
    "    return 2+X**4\n",
    "\n",
    "n_samples = 25\n",
    "n_degrees = 15\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X_train = np.random.normal(size=(n_samples,1))\n",
    "y_train = true_fct(X_train).reshape(-1) + np.random.randn(n_samples) \n",
    "\n",
    "X_test = np.random.normal(size=(n_samples,1))\n",
    "y_test = true_fct(X_test).reshape(-1) + np.random.randn(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a polynomial (3)\n",
    "We estimate the polynomials and store MSE for train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "test_mse = []\n",
    "train_mse = []\n",
    "parameters = []\n",
    "degrees = range(n_degrees+1)\n",
    "\n",
    "for p in degrees:\n",
    "    X_train_p = PolynomialFeatures(degree=p).fit_transform(X_train)\n",
    "    X_test_p = PolynomialFeatures(degree=p).fit_transform(X_test)\n",
    "    reg = LinearRegression().fit(X_train_p, y_train)\n",
    "    train_mse += [mse(reg.predict(X_test_p),y_test)] \n",
    "    test_mse += [mse(reg.predict(X_test_p),y_test)]     \n",
    "    parameters.append(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a polynomial (4)\n",
    "*So what happens to the model performance in- and out-of-sample?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_index = pd.Index(degrees,name='Polynomial degree ~ model complexity')\n",
    "#'Test set':test_mse\n",
    "ax = pd.DataFrame({'Train set':train_mse, 'Test set':test_mse})\\\n",
    "    .set_index(degree_index).plot(figsize=(14,5), logy=True)\n",
    "ax.set_ylabel('Mean squared error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a polynomial (4)\n",
    "*Why does it go wrong on the test data?*\n",
    "- more spurious parameters\n",
    "- the coefficient size increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a polynomial (5)\n",
    "*What do you mean coefficient size increase?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_idx = pd.Index(range(n_degrees+1),name='Polynomial order')\n",
    "ax = pd.DataFrame(parameters,index=order_idx)\\\n",
    ".abs().mean(1).plot(figsize=(14,5),logy=True)\n",
    "ax.set_ylabel('Mean parameter size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a polynomial (6)\n",
    "*How else could we visualize this problem?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f_bias_var['regression'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Curing the overfitting curse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Looking for a remedy\n",
    "*How might we solve the overfitting problem?*\n",
    "\n",
    "By reducing\n",
    "- the number of variables\n",
    "- the coefficient size of variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (1)\n",
    "\n",
    "*Why do we regularize?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To mitigate overfitting > better model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*How do we regularize?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We make models which are less complex:\n",
    "  - reducing the **number** of coefficient;\n",
    "  - reducing the **size** of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (2)\n",
    "\n",
    "*What does regularization look like?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We add a penalty term our optimization procedure:\n",
    "    \n",
    "$$ \\text{arg min}_\\beta \\, \\underset{\\text{MSE}}{\\underbrace{E[(y_0 - \\hat{f}(x_0))^2]}} + \\underset{\\text{penalty}}{\\underbrace{\\lambda \\cdot R(\\beta)}}$$\n",
    "\n",
    "Introduction of penalties implies that increased model complexity has to be met with high increases precision of estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (3)\n",
    "\n",
    "*What are some used penalty functions?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The two most common penalty functions are L1 and L2 regularization.\n",
    "\n",
    "- L1 regularization (***Lasso***): $R(\\beta)=\\sum_{j=1}^{p}|\\beta_j|$ \n",
    "    - Makes coefficients sparse, i.e. selects variables by removing some (if $\\lambda$ is high)\n",
    "    \n",
    "    \n",
    "- L2 regularization (***Ridge***): $R(\\beta)=\\sum_{j=1}^{p}\\beta_j^2$\n",
    "    - Reduce coefficient size\n",
    "    - Fast due to analytical solution\n",
    "    \n",
    "*To note:* The *Elastic Net* uses a combination of L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (4)\n",
    "\n",
    "*How the Lasso (L1 reg.) deviates from OLS*\n",
    "\n",
    "<center><img src='http://rasbt.github.io/mlxtend/user_guide/general_concepts/regularization-linear_files/l1.png' alt=\"Drawing\" style=\"width: 550px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (5)\n",
    "\n",
    "*How the Ridge regression (L2 reg.) deviates from OLS*\n",
    "\n",
    "<center><img src='http://rasbt.github.io/mlxtend/user_guide/general_concepts/regularization-linear_files/l2.png' alt=\"Drawing\" style=\"width: 550px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (6)\n",
    "\n",
    "*How might we describe the $\\lambda$ of Lasso and Ridge?*\n",
    "\n",
    "These are hyperparameters that we can optimize over. \n",
    "\n",
    "- More about this tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (7)\n",
    "\n",
    "*Is there a generalization of of Lasso and Ridge?*\n",
    "\n",
    "Yes, the elastic net allows both types of regularization. Thererfore, it has two hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The devils in the details (1)\n",
    "\n",
    "*So we just run regularization?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need to rescale our features:\n",
    "- convert to zero mean: \n",
    "- standardize to unit std: \n",
    "\n",
    "Compute in Python:\n",
    "- option 1: `StandardScaler` in `sklearn` (RECOMMENDED)\n",
    "- option 2: `(X - np.mean(X)) / np.std(X)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The devils in the details (2)\n",
    "*So we just scale our test and train?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fit to the distribution in the training data first, then rescale train and test! See more [here](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The devils in the details (3)\n",
    "*So we just rescale before using polynomial features?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Otherwise the interacted varaibles are not gaussian distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The devils in the details (4)\n",
    "*Does sklearn's `PolynomialFeatures` work for more than variable?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# YES!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Curing the underfitting curse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting remedies\n",
    "*Is it possible to solve the underfitting problem?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there are in general two ways.\n",
    "- Using polynomial interactions of all features.\n",
    "    - This is known as Taylor expansion\n",
    "    - Note: we need to use regularization too curb impact on overfitting!\n",
    "- Using non-linear models who can capture all patterns.\n",
    "    - These are called universal approximators\n",
    "    - Return to an overview of these in Session 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting remedies (2)\n",
    "*Some of the models we see here, e.g. Perceptrons, seem too simple - are they ever useful?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No, not for serious machine learning. \n",
    "- But for exposition (your learning), yes.\n",
    "- However, the perceptron and related models are building blocks for building neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The end\n",
    "[Return to agenda](#Agenda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Code for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%run ../ML_plots.ipynb"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
