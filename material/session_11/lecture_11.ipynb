{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Notebook contents: \n",
    "\n",
    "This notebook contains a lecture. The code for generating plots are found at the of the notebook. Links below.\n",
    "\n",
    "- [presentation](#Session-1b:)\n",
    "- [code for plots](#Code-for-plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 11:\n",
    "## Machine learning introduction\n",
    "\n",
    "*Andreas Bjerre-Nielsen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taking stock\n",
    "\n",
    "*What have we learned until now?*\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some coding advice\n",
    "\n",
    "- *How do I extract an object from my function?* Print or return?\n",
    "- Solving complex problems: One thing at a time\n",
    "- Is joining datasets difficult? \n",
    "    - Check out [the pandas documentation on merging](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n",
    "    - Or [the guide available from Jake Van der Plas](https://jakevdp.github.io/PythonDataScienceHandbook/03.07-merge-and-join.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. [Math and stats review](#Math-review)\n",
    "1. [Why machine learing](#Why-machine-learning)\n",
    "1. [What is machine learning](#Machine-learning-overview)\n",
    "1. Classification models\n",
    "    1. [the perceptron](#The-perceptron-model)\n",
    "    1. [beyond the perceptron](#Beyond-the-perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Math review\n",
    "\n",
    "Vector: 1-d dimensional array of numbers \n",
    "\\begin{align}\\boldsymbol{x}=[x_0,x_1,x_2,..]\\end{align}\n",
    "\n",
    "<br>\n",
    "\n",
    "Matrix: 2-d dimensional array of numbers \n",
    "\\begin{eqnarray}\\boldsymbol{X}=[\n",
    "[&x_{00}&,x_{01}&,x_{02}&,..&],\\\\\n",
    "[&x_{10}&,x_{11}&,x_{12}&,..&],\\\\\n",
    "[&x_{20}&,x_{21}&,x_{22}&,..&],\\\\\n",
    "[&...   &,...   &,...   &,..&]]\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Function fitting\n",
    "*What does (supervised) machine learning do?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have some data $y$ we want to model/predict from input $x$.  \n",
    "\n",
    "The aim is to find a function $f$ such that the distance between actual values $y$ and predicted values $f(x)$ are minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*What are some Examples?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Linear form: $y=x\\beta$.\n",
    "- Logistic form: $y=g(x\\beta)$\n",
    "\n",
    "where $x^T\\beta=\\beta_0+x_1\\beta_1+x_2\\beta_2+...+x_n\\beta_n$ (vector dot product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of modelling \n",
    "*Why are models useful?*\n",
    "\n",
    "Models are pursued with differens aims. Suppose we have a linear model, $y=x\\beta+\\epsilon$.\n",
    "\n",
    "- Social science:\n",
    "    - They teach us something about the world.\n",
    "    - We want to estimate $\\hat{\\beta}$ and distribution\n",
    "- Data science:\n",
    "    - To make optimal future decisions and precise predictions, i.e. $\\hat{y}$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model fragility (1)\n",
    "*What is a polynomial regression?*\n",
    "\n",
    "- Fitting a curve with an *n-dimenstional polynomial*\n",
    "- Can fit any \"regular\" curve ~ Taylor Series Approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model fragility (2)\n",
    "*Suppose we build models of the size of the Danish population, how do polynomial fits perform?*\n",
    "- We estimate model with data from 1769-1975."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_pop1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-72c9ccb5ed87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf_pop1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'f_pop1' is not defined"
     ]
    }
   ],
   "source": [
    "f_pop1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model fragility (3)\n",
    "*Which model performs best when we extend the forecasting period from 1975 to now?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pop2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model fragility (4)\n",
    "*What happens if we extend the prediction period until 2050? See the fifth order.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pop3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model fragility (5)\n",
    "*What trade off do we face in modelling?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Making a model that is too simple and does not capture enough of data (`underfitting`)\n",
    "- Making a model with great fit on estimation data, but poor out-of-sample prediction (`overfitting`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal of machine learning is to find models that minimize these two problems simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning ML\n",
    "\n",
    "- During lectures copy code for see what it does - ***listen*** to me. Write own notes.\n",
    "- After lecture > understand code details\n",
    "- Learn with your group - VERY IMPORTANT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning outline for this course \n",
    "\n",
    "ML: short for machine learning\n",
    "\n",
    "- Problems: ***supervised*** vs unsupervised\n",
    "- Linear supervised ML models \n",
    "    - classification and regression\n",
    "    - regularization \n",
    "    - **getting hands dirty with implementing solver**\n",
    "- Fundamental concepts of ML\n",
    "    - overfitting, underfitting, model validation\n",
    "    - model selection and hyperparameters\n",
    "- Emphasize differences and synergies between ML and statistics\n",
    "- Brief intro of non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is machine learning\n",
    "*Can you define machine learning, i.e. ML?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Supervised learning\n",
    "  - Models designed to infer a relationship between input and **labeled** data.      \n",
    "  - We define the `target` as labels in data we wish to model. \n",
    "      - Example: population as a function of year. \n",
    "- Unsupervised learning\n",
    "  - Find patterns and relationships from **unlabeled** data. \n",
    "  - This may involve clustering, dimensionality reduction and more.  \n",
    "  - Not part of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why machine learning \n",
    "*How might this be useful for social scientists?*\n",
    "\n",
    "Supervised machine learning is important (elaborated in Lecture 14):\n",
    "- Improve estimation by validating models (not only theory)\n",
    "- We can generate new data (impute missing)\n",
    "- Better predictive models \n",
    "- Use in hybrid models that leverage machine learning for causal estimation \n",
    "    - (e.g. causal forest, neural instrumentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised ML problems \n",
    "*How can we categorize a supervised ML model?*\n",
    "\n",
    "Suppose we have model $y=g(X\\beta)$\n",
    "\n",
    "We distinguish by type of the `target` variable `y`:\n",
    "- **regression**: predict a numeric value\n",
    "- **classification**: distinguish between target categories (non-numeric data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised ML problems (2)\n",
    "*Which one is classification, which one is regression?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_identify_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised ML problems (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_identify_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression models\n",
    "*What are examples of regressions models?*\n",
    "\n",
    "- Example of targets: income, life expectancy, education length (years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*What is the underlying data of the target, $y$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- target is `continuous` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classications models\n",
    "*What are examples of classication models?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*What is the underlying data of the target, $y$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Targets are categories \n",
    "  - sometimes known as `factor` in statistics \n",
    "  - (work for `str`, `bool`,  `int`, `float` which are then interpreted as categories)\n",
    "- Examples of target: kind of education (linguistics, math), mode of transportation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of supervised ML\n",
    "*Classification or regression?*\n",
    "\n",
    "We load the titanic data. We select variables and make dummy variables from categorical. We split into target and features. \n",
    "\n",
    "Target is: ...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "cols = ['survived','class', 'sex', 'sibsp', 'age', 'alone']\n",
    "titanic_sub = pd.get_dummies(titanic[cols].dropna(), drop_first=True).astype(np.int64)  \n",
    "\n",
    "X = titanic_sub.drop('survived', axis=1)\n",
    "y = titanic_sub.survived\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions\n",
    "\n",
    "ML lingo and econometric equivalents\n",
    "\n",
    "- `feature` vector, $\\textbf{x}_i$, i.e a row of input variables\n",
    "  - = explanatory **variables** in econometrics\n",
    "- `weight` vector, $\\textbf{w}$, i.e model parameters\n",
    "  - = **coefficients** in econometrics where denoted $\\beta$\n",
    "- `bias` term, $w_0$, i.e. the model intercept\n",
    "  - = the **constant** variable in denoted $\\beta_0$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The perceptron model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## The articifial neuron\n",
    "\n",
    "A real neuron maps stimulus (input) to output. \n",
    "\n",
    "[Research estimates](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5063692) there are 55‚Äì70 billion neurons in the brain.\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch02/images/02_01.png' alt=\"Drawing\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The articifial neuron (2)\n",
    "We are interested in making a decision rule that takes arbitrary input and outputs either positive or negative. \n",
    "\n",
    "Mathematically we define this map as $\\phi: \\mathbb{R}^p\\rightarrow\\{-1, 1\\}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\phi(z_i)=\\begin{cases}\n",
    "\\hfill1, & z_i>0\\\\\n",
    "-1, & z_i\\le0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "- `net-input`, $z_i = \\underset{~vector\\,form}{\\underbrace{\\boldsymbol{w}^{T}\\boldsymbol{x}_i}} = \\underset{~expanded\\,form}{\\underbrace{1\\cdot w_0+w_1x_{i,1}+...+w_kx_{i,k}}}$\n",
    "\n",
    "- `unit step function`, $\\phi$, checks if value exceeds threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The articifial neuron (3)\n",
    "Quiz: what are the input dimensions of the neuron, what is the output dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Input is the p-dimensional space, $\\mathbb{R}^p$.\n",
    "- Output is binary, either $-1$ or $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The articifial neuron (4)\n",
    "*The unit step function (left) and the decision boundary (right)*\n",
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch02/images/02_02.png' alt=\"Drawing\" style=\"width: 1000px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The articifial neuron (5)\n",
    "*When does the articial neuron work?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If the two target types are linearly separable:\n",
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch02/images/02_03.png' alt=\"Drawing\" style=\"width: 1200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The perceptron learning rule (1)\n",
    "*How do we estimate the model parameters?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. initialize the weight with small random number\n",
    "1. for each training observation, i=1,..,n\n",
    "  1. compute predicted target, $\\hat{y}_i$\n",
    "  1. update weights $\\hat{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The perceptron learning rule (2)\n",
    "*How do we compute the predicted target $\\hat{y}$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We apply a transformation on the net-input :\n",
    "- single observation, expanded notation:\n",
    "\\begin{align*}\n",
    "\\hat{y}_i= \\phi(z_i),\\quad z_i=w_0+w_1x_{i,1}+...+w_kx_{i,k}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- single observation, vector notation:\n",
    "\\begin{align*}\n",
    "\\hat{y}_i= \\phi(z_i),\\quad z_i=\\boldsymbol{w}^{T}\\boldsymbol{x}_i\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- multiple observations, matrix notation:\n",
    "\\begin{align*}\n",
    "\\hat{\\boldsymbol{y}}= & \\phi(\\boldsymbol{z}),\\quad\\boldsymbol{z}=\\boldsymbol{X}\\boldsymbol{w}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The perceptron learning rule (3)\n",
    "*How do we update weights?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Weights are updated as follows:\n",
    "\\begin{align*}\n",
    "w&=w+\\Delta w\\\\\n",
    "\\Delta w&=\\eta\\cdot(y_i-\\phi(z_i))\\cdot \\textbf{x}_{i}\\end{align*}\n",
    "\n",
    "where $\\eta$ is the learning rate, and the first order derivative is:\n",
    "\n",
    "$$\\frac{\\partial SSE}{\\partial w}=- \\textbf{X}^T\\textbf{e}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The perceptron learning rule (4)\n",
    "\n",
    "The computation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch02/images/02_04.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation in Python (1)\n",
    "*Let's set some values of input and output* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(size=(3, 2)) # feature matrix\n",
    "y = np.array([1, -1, 1]) # target vector\n",
    "w = np.random.normal(size=(3)) # weight vector\n",
    "print('X:\\n',X)\n",
    "print('y:',y)\n",
    "print('w:',w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation in Python (2)\n",
    "*How do we compute the errors vectorized?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute net-input\n",
    "z = w[0] + X.dot(w[1:]) # w[0]): is our bias, X: is our features and w[1:]: is our weights\n",
    "\n",
    "# unit step-function\n",
    "predict_pos = z > 0 # compute prediction (boolean)\n",
    "y_hat = np.where(predict_pos, 1, -1)  # convert prediction\n",
    "\n",
    "# compute errors\n",
    "e = y - y_hat # compute errors\n",
    "SSE = e.T.dot(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation in Python (3)\n",
    "*How do we compute the updated weights?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# learning rate\n",
    "eta = 0.001 \n",
    "\n",
    "# update weights \n",
    "update_vars = eta*X.T.dot(e) # insert fod \n",
    "update_bias = eta*e.sum()/2\n",
    "\n",
    "# negative first order derivative (FOD) of SSE wrt ùõΩ\n",
    "# FOD vector notation: = -ùúÄ'X = -(Y-XùõΩ)'X  \n",
    "#fod = X.T.dot(e) / 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with the perceptron (1)\n",
    "We load the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris').iloc[:100] # drop virginica\n",
    "\n",
    "X = iris.iloc[:, [0, 2]].values # keep petal_length and sepal_length\n",
    "y = np.where(iris.species=='setosa', 1, -1) # convert to 1, -1\n",
    "\n",
    "sns.scatterplot(iris.sepal_length, iris.petal_length, hue=iris.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with the perceptron (2)\n",
    "*How do we fit the perceptron model?* [perceptron definition](#Code-from-Raschka-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the perceptron\n",
    "clf = Perceptron(n_iter=10) # clf: short for calssifier (classificaiton model), n_iter is number of iterations.\n",
    "\n",
    "# fit the perceptron\n",
    "# runs 10 iterations of updating the model\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with the perceptron (3)\n",
    "*How can we evaluate the model??*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of errors: %i' % sum(clf.predict(X)!=y))\n",
    "\n",
    "# we plot the decisions\n",
    "plot_decision_regions(X,y,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with the perceptron (4)\n",
    "*How does the model performance change??*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(12, 4))\n",
    "ax.set_xticks(range(11))\n",
    "ax.plot(range(1, len(clf.errors_) + 1), clf.errors_, marker='o')\n",
    "ax.set_xlabel('Number of iterations')\n",
    "ax.set_ylabel('Number of errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model validation\n",
    "*How can we see how our model generalizes?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can simulate out-of-sample prediction. How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "- Idea: Use some of our sample for model evaluation.\n",
    "- Implementation - divide data randomly into two subsets:\n",
    "    - `training data` for estimation; \n",
    "    - `test data` for evaluation.\n",
    "- Note: does not work for time series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model validation (2)\n",
    "We revert to titanic, `y`: survived, `X`: everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(titanic_sub.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We split the data into test and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0) # random_state is like a seed. Make sure it is the same for all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "*What might we change about the perceptron?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Change from updating errors that are binary to continuous\n",
    "2. Use more than one observation a time for updating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The activation function (1)\n",
    "*What else might we use to update errors?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The most simple is **no transformation** of the net-input, i.e. $\\phi(z_i)=z_i$.\n",
    "\n",
    "- When we change this from perceptron we call it Adaptive Linear Neuron (**Adaline**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The activation function (2)\n",
    "*How is this different from the Perceptron?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch02/images/02_09.png' alt=\"Drawing\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The activation function (3)\n",
    "*Which activation functions can be used?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Linear \n",
    "- Logistic (Sigmoid)\n",
    "- Unit step, sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "See page 450 in Python for Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The activation function (4)\n",
    "*How do Adaline and Logistic regression differ?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch03/images/03_03.png' alt=\"Drawing\" style=\"width: 600px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A new objective (1)\n",
    "*The update rule in perceptron seems ad hoc, is there a more general way?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yes, we minimize the sum of squared errors (SSE). The SSE for Adaline is:\n",
    "\\begin{align}SSE&=\\boldsymbol{e}^{T}\\boldsymbol{e}=e_1^2+..+e_n^2\\\\\\boldsymbol{e}&=\\textbf{y}-\\textbf{X}\\textbf{w}\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Doesn't the above look strangely familiar?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yes, it is the same objective as OLS. The difference:\n",
    "    - OLS computes the exact solution with system of equations from first order conditions.\n",
    "    - We make an approximate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A new objective (2)\n",
    "*So how the hell do we make the approximate solution?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Two general classes:\n",
    "  -  We approximate the first order derivative ~ gradient descent (GD)\n",
    "  -  We approximate both first and second order derivative ~ quasi Newton    \n",
    " <br />\n",
    "- We take gradient descent - much simpler (often faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A new objective (3)\n",
    "*How does a gradient descent look?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An algorithm that finds the direction where expected differences are largest. Attempt of satisfying first order condition (FOC).\n",
    "\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/7/79/Gradient_descent.png' alt=\"Drawing\" style=\"width: 350px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A new objective (4)\n",
    "*What is the first order derivative of SSE wrt. weights in Adaline?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align}\\frac{\\partial SSE}{\\partial w}=\\textbf{X}^T\\textbf{e},\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "*How do we update with GD in Adaline?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Idea: take small steps to approximate the solution.\n",
    "\n",
    "  - $\\Delta w=\\eta\\textbf{X}^T\\textbf{e}=\\eta\\cdot\\textbf{X}^T(\\textbf{y}-\\hat{\\textbf{y}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A new objective (5)\n",
    "The gradient descent algorithm we just learned uses the whole data.\n",
    "\n",
    "- Often known as batch gradient descent.\n",
    "\n",
    "*What might be a smart way of changing (batch) gradient descent?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We only use a subset of the data. Two variants:\n",
    "    - *stochastic gradient descent* (SGD): uses random subset of observations\n",
    "    - *mini batch*: uses deterministic subset of observations (loop whole dataset)\n",
    "    \n",
    "- Idea: we converge faster by computing update for subset of data\n",
    "    - Note: we may need a million repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying logistic regression\n",
    "*How difficult is it to use `LogisticRegression`?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# estimate model on train data, evaluate on test data\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(X_train, y_train) # model training\n",
    "y_hat = clf.predict(X_test)\n",
    "#accuracy = (y_hat==y_test).mean() \n",
    "accuracy = accuracy_score(y_test, y_hat)# model testing\n",
    "print('Model accuracy is:', np.round(accuracy,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The end\n",
    "[Return to agenda](#Agenda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Code for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 4 # set default size of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Population plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%run pop_plots.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Plots of ML types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%run ../ML_plots.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Plots from book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "base_url = 'https://raw.githubusercontent.com/rasbt/python-machine-learning-book-2nd-edition/master/code/ch02/'\n",
    "\n",
    "for filename in ('ch02.py', 'iris.data', 'iris.names.txt'):\n",
    "    if not os.path.exists(filename):\n",
    "        response = requests.get(base_url+filename)\n",
    "        with open(filename,'wb') as f:\n",
    "            f.write(response.text.encode('utf-8'))\n",
    "    \n",
    "from ch02 import Perceptron, AdalineGD, AdalineSGD, plot_decision_regions"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
